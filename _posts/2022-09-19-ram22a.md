---
title: On the Optimality Gap of Warm-Started Hyperparameter Optimization
openreview: SUQgACdBfx5
abstract: We study the general framework of warm-started hyperparameter optimization
  (HPO) where we have some source datasets (tasks) where we have already performed
  HPO, and we wish to leverage the results of these HPO to warm-start the HPO on an
  unseen target dataset (and perform few-shot HPO). Various meta-learning schemes
  have been proposed over the last decade (and more) for this problem. In this paper,
  we theoretically analyse the optimality gap of the hyperparameter obtained via such
  warm-started few-shot HPO, and provide novel results for multiple existing meta-learning
  schemes. We show how these results allow us identify situations where certain schemes
  have advantage over others.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ram22a
month: 0
tex_title: On the Optimality Gap of Warm-Started Hyperparameter Optimization
firstpage: 12/1
lastpage: 14
page: 12/1-14
order: 12
cycles: false
bibtex_author: Ram, Parikshit
author:
- given: Parikshit
  family: Ram
date: 2022-09-19
address:
container-title: Proceedings of the First International Conference on Automated Machine
  Learning
volume: '188'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 9
  - 19
pdf: https://proceedings.mlr.press/v188/ram22a/ram22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
