---
title: Non-Uniform Adversarially Robust Pruning
openreview: HZVex1EXSe9
abstract: Neural networks often are highly redundant and can thus be effectively compressed
  to a fraction of their initial size using model pruning techniques without harming
  the overall prediction accuracy. Additionally, pruned networks need to maintain
  robustness against attacks such as adversarial examples. Recent research on combining
  all these objectives has shown significant advances using uniform compression strategies,
  that is, all weights or channels are compressed equally according to a preset compression
  ratio. In this paper, we show that employing non-uniform compression strategies
  allows to significantly improve clean data accuracy as well as adversarial robustness
  under high overall compression. We leverage reinforcement learning for finding an
  optimal trade-off and demonstrate that the resulting compression strategy can be
  used as a plug-in replacement for uniform compression ratios of existing state-of-the-art
  approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao22a
month: 0
tex_title: Non-Uniform Adversarially Robust Pruning
firstpage: 1/1
lastpage: 16
page: 1/1-16
order: 1
cycles: false
bibtex_author: Zhao, Qi and K\"onigl, Tim and Wressnegger, Christian
author:
- given: Qi
  family: Zhao
- given: Tim
  family: KÃ¶nigl
- given: Christian
  family: Wressnegger
date: 2022-09-19
address:
container-title: Proceedings of the First International Conference on Automated Machine
  Learning
volume: '188'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 9
  - 19
pdf: https://proceedings.mlr.press/v188/zhao22a/zhao22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
