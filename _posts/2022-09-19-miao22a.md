---
title: Differentiable Architecture Search for Reinforcement Learning
openreview: HOZx3iEht15
abstract: 'In this paper, we investigate the fundamental question: To what extent
  are gradient-based neural architecture search (NAS) techniques applicable to RL?
  Using the original DARTS as a convenient baseline, we discover that the discrete
  architectures found can achieve up to 250% performance compared to manual architecture
  designs on both discrete and continuous action space environments across off-policy
  and on-policy RL algorithms, at only 3{\texttimes} more computation time. Furthermore,
  through numerous ablation studies, we systematically verify that not only does DARTS
  correctly upweight operations during its supernet phrase, but also gradually improves
  resulting discrete cells up to 30{\texttimes} more efficiently than random search,
  suggesting DARTS is surprisingly an effective tool for improving architectures in
  RL.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: miao22a
month: 0
tex_title: Differentiable Architecture Search for Reinforcement Learning
firstpage: 20/1
lastpage: 17
page: 20/1-17
order: 20
cycles: false
bibtex_author: Miao, Yingjie and Song, Xingyou and Co-Reyes, John D and Peng, Daiyi
  and Yue, Summer and Brevdo, Eugene and Faust, Aleksandra
author:
- given: Yingjie
  family: Miao
- given: Xingyou
  family: Song
- given: John D
  family: Co-Reyes
- given: Daiyi
  family: Peng
- given: Summer
  family: Yue
- given: Eugene
  family: Brevdo
- given: Aleksandra
  family: Faust
date: 2022-09-19
address:
container-title: Proceedings of the First International Conference on Automated Machine
  Learning
volume: '188'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 9
  - 19
pdf: https://proceedings.mlr.press/v188/miao22a/miao22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
