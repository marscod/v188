@Proceedings{automlconf22,
  booktitle         = {Proceedings of the First International Conference on Automated Machine Learning},
  name              = {International Conference on Automated Machine Learning},
  shortname         = {AutoML-Conf},
  year              = {2022},
  editor            = {Guyon, Isabelle and Lindauer, Marius and van der Schaar, Mihaela and Hutter, Frank and Garnett, Roman},
  volume            = {188},
  start             = {2022-07-25},
  end               = {2022-07-27},
  published               = {2022-09-19},
  address           = {Johns Hopkins University, Baltimore, MD, USA},
  conference_url    = {https://automl.cc/conferences-2022/},
  conference_number = {1}
}

@InProceedings{zhao22,
  title      = {Non-Uniform Adversarially Robust Pruning},
  author    = {Zhao, Qi and K\"onigl, Tim and Wressnegger, Christian},
  pages      = {1/1--16},
  openreview = {HZVex1EXSe9},
  abstract   = {Neural networks often are highly redundant and can thus be effectively compressed to a fraction of their initial size using model pruning techniques without harming the overall prediction accuracy. Additionally, pruned networks need to maintain robustness against attacks such as adversarial examples. Recent research on combining all these objectives has shown significant advances using uniform compression strategies, that is, all weights or channels are compressed equally according to a preset compression ratio. In this paper, we show that employing non-uniform compression strategies allows to significantly improve clean data accuracy as well as adversarial robustness under high overall compression. We leverage reinforcement learning for finding an optimal trade-off and demonstrate that the resulting compression strategy can be used as a plug-in replacement for uniform compression ratios of existing state-of-the-art approaches.}
}

@InProceedings{sehic22,
  title      = {LassoBench: A High-Dimensional Hyperparameter Optimization Benchmark Suite for Lasso},
  author    = {\v{S}ehi\'c, Kenan and Gramfort, Alexandre and Salmon, Joseph and Nardi, Luigi},
  pages      = {2/1--24},
  openreview = {HU7eP2fR7lc},
  abstract   = {While Weighted Lasso sparse regression has appealing statistical guarantees that would entail a major real-world impact in finance, genomics, and brain imaging applications, it is typically scarcely adopted due to its complex high-dimensional space composed by thousands of hyperparameters. On the other hand, the latest progress with high-dimensional hyperparameter optimization (HD-HPO) methods for black-box functions demonstrates that high-dimensional applications can indeed be efficiently optimized. Despite this initial success, HD-HPO approaches are mostly applied to synthetic problems with a moderate number of dimensions, which limits its impact in scientific and engineering applications. We propose LassoBench, the first benchmark suite tailored for Weighted Lasso regression. LassoBench consists of benchmarks for both well-controlled synthetic setups (number of samples, noise level, ambient and effective dimensionalities, and multiple fidelities) and real-world datasets, which enables the use of many flavors of HPO algorithms to be studied and extended to the high-dimensional Lasso setting. We evaluate 6 state-of-the-art HPO methods and 3 Lasso baselines, and demonstrate that Bayesian optimization and evolutionary strategies can improve over the methods commonly used for sparse regression while highlighting limitations of these frameworks in very high-dimensional and noisy settings.}
}

@InProceedings{pfisterer22,
  title      = {YAHPO Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization},
  author    = {Pfisterer, Florian and Schneider, Lennart and Moosbauer, Julia and Binder, Martin and Bischl, Bernd},
  pages      = {3/1--39},
  openreview = {H2zlL3EXBxq},
  software   = {https://github.com/slds-lmu/yahpo_gym},
  abstract   = {When developing and analyzing new hyperparameter optimization methods, it is vital to empirically evaluate and compare them on well-curated benchmark suites. In this work, we propose a new set of challenging and relevant benchmark problems motivated by desirable properties and requirements for such benchmarks. Our new surrogate-based benchmark collection consists of 14 scenarios that in total constitute over 700 multi-fidelity hyperparameter optimization problems, which all enable multi-objective hyperparameter optimization. Furthermore, we empirically compare surrogate-based benchmarks to the more widely-used tabular benchmarks, and demonstrate that the latter may produce unfaithful results regarding the performance ranking of HPO methods. We examine and compare our benchmark collection with respect to defined requirements and propose a single-objective as well as a multi-objective benchmark suite on which we compare 7 single-objective and 7 multi-objective optimizers in a benchmark experiment.  Our software is available at \url{https://github.com/slds-lmu/yahpo_gym}.}
}

@InProceedings{hoang22,
  title      = {AutoCoG: A Unified Data-Model Co-Search Framework for Graph Neural Networks},
  author    = {Hoang, Duc N.M and Zhou, Kaixiong and Chen, Tianlong and Hu, Xia and Wang, Zhangyang},
  pages      = {4/1--16},
  openreview = {rU9xzLgFre5},
  abstract   = {Neural architecture search (NAS) has demonstrated success in discovering promising architectures for vision or language modeling tasks, and it has recently been introduced to searching for graph neural networks (GNNs) as well. Despite the preliminary success, GNNs struggle in dealing with heterophily or low-homophily graphs where connected nodes may have different class labels and dissimilar features. To this end, we propose co-optimizing both the input graph topology and the model's architecture topology simultaneously. That yields AutoCoG, the first unified data-model co-search NAS framework for GNNs. By defining a highly flexible data-model co-search space, AutoCoG is gracefully formulated as a principled bi-level optimization that can be end-to-end solved by the differentiable search methods. Experiments show AutoCoG achieves gains of up to 4\% for Actor, 7.3\% on average for Web datasets, 0.17\% for CoAuthor-CS, and finally 5.4\% for Wikipedia-Photo benchmarks. All codes will be released upon paper acceptance.}
}

@InProceedings{munoz22,
  title      = {Automated Super-Network Generation for Scalable Neural Architecture Search},
  author    = {Munoz, Juan Pablo and Lyalyushkin, Nikolay and Lacewell, Chaunte Willetta and Senina, Anastasia and Cummings, Daniel and Sarah, Anthony and Kozlov, Alexander and Jain, Nilesh},
  pages      = {5/1--15},
  openreview = {rImlRtoyLl9},
  abstract   = {Weight-sharing Neural Architecture Search (NAS) solutions often discover neural network architectures that outperform their human-crafted counterparts. Weight-sharing allows the creation and training of super-networks that contain many smaller and more efficient child models, a.k.a., sub-networks. For an average deep learning practitioner, generating and training one of these super-networks for an arbitrary neural network architecture design space can be a daunting experience. In this paper, we present BootstrapNAS, a software framework that addresses this challenge by automating the generation and training of super-networks. Developers can use this solution to convert a pre-trained model into a super-network. BootstrapNAS then trains the super-network using a weight-sharing NAS technique available in the framework or provided by the user. Finally, a search component discovers high-performing sub-networks that are returned to the end-user. We demonstrate BootstrapNAS by automatically generating super-networks from popular pre-trained models (MobileNetV2, MobileNetV3, EfficientNet, ResNet50 and HyperSeg), available from Torchvision and other repositories. BootstrapNAS can achieve up to 9.87{\texttimes} improvement in throughput in comparison to the pre-trained Torchvision ResNet-50 (FP32) on Intel Xeon platform.}
}

@InProceedings{pulatov22,
  title      = {Opening the Black Box: Automated Software Analysis for Algorithm Selection},
  author    = {Pulatov, Damir and Anastacio, Marie and Kotthoff, Lars and Hoos, Holger},
  pages      = {6/1--18},
  openreview = {Sgg5KAwNg9},
  abstract   = {Impressive performance improvements have been achieved in many areas of AI by meta-algorithmic techniques, such as automated algorithm selection and configuration. However, existing techniques treat the target algorithms they are applied to as black boxes -- nothing is known about their inner workings. This allows meta-algorithmic techniques to be used broadly, but leaves untapped potential performance improvements enabled by information gained from a deeper analysis of the target algorithms. In this paper, we open the black box without sacrificing universal applicability of meta-algorithmic techniques by automatically analyzing algorithms. We show how to use this information to perform algorithm selection, and demonstrate improved performance compared to previous approaches that treat algorithms as black boxes.}
}

@InProceedings{makarova22,
  title      = {Automatic Termination for Hyperparameter Optimization},
  author    = {Makarova, Anastasia and Shen, Huibin and Perrone, Valerio and Klein, Aaron and Faddoul, Jean Baptiste and Krause, Andreas and Seeger, Matthias and Archambeau, Cedric},
  pages      = {7/1--21},
  openreview = {rAflwNFGUxq},
  abstract   = {Bayesian optimization (BO) is a widely popular approach for the hyperparameter optimization (HPO) in machine learning.  At its core, BO iteratively evaluates promising configurations until a user-defined budget, such as wall-clock time or number of iterations, is exhausted. While the final performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an effective and intuitive termination criterion for BO that automatically stops  the procedure if it is sufficiently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error. Across an extensive range of real-world HPO problems and baselines, we show that our termination criterion achieves a better trade-off between the test performance and optimization time. Additionally, we find that overfitting may occur in the context of HPO, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets.}
}

@InProceedings{song22,
  title      = {Open Source Vizier: Distributed Infrastructure and API for Reliable and Flexible Blackbox Optimization},
  author    = {Song, Xingyou and Perel, Sagi and Lee, Chansoo and Kochanski, Greg and Golovin, Daniel},
  pages      = {8/1--17},
  openreview = {rOOgZaMl7x9},
  software   = {https://github.com/google/vizier},
  abstract   = {Vizier is the de-facto blackbox optimization service across Google, having optimized some of Google's largest products and research efforts. To operate at the scale of tuning thousands of users' critical systems, Vizier solved key design challenges in providing multiple different features, while remaining fully fault-tolerant. In this paper, we introduce Open Source (OSS) Vizier, a Python-based interface for blackbox optimization and research, based on the Google-internal Vizier infrastructure and framework. OSS Vizier provides an API capable of defining and solving a wide variety of optimization problems, including multi-metric, early stopping, transfer learning, and conditional search. Furthermore, it is designed to be a distributed system that assures reliability, and allows multiple parallel evaluations of the user's objective function. The flexible RPC-based infrastructure allows users to access OSS Vizier from binaries written in any language. OSS Vizier also provides a back-end (''Pythia'') API that gives algorithm authors a way to interface new algorithms with the core Vizier system. OSS Vizier is available at \url{https://github.com/google/vizier}.}
}

@InProceedings{schneider22,
  title      = {Tackling Neural Architecture Search With Quality Diversity Optimization},
  author    = {Schneider, Lennart and Pfisterer, Florian and Kent, Paul and Branke, Juergen and Bischl, Bernd and Thomas, Janek},
  pages      = {9/1--30},
  openreview = {BAzgqswmSx9},
  abstract   = {Neural architecture search (NAS) has been studied extensively and has grown to become a research field with substantial impact. While classical single-objective NAS searches for the architecture with the best performance, multi-objective NAS considers multiple objectives that should be optimized simultaneously, e.g., minimizing resource usage along the validation error. Although considerable progress has been made in the field of multi-objective NAS, we argue that there is some discrepancy between the actual optimization problem of practical interest and the optimization problem that multi-objective NAS tries to solve. We resolve this discrepancy by formulating the multi-objective NAS problem as a quality diversity optimization (QDO) problem and introduce three quality diversity NAS optimizers (two of them belonging to the group of multifidelity optimizers), which search for high-performing yet diverse architectures that are optimal for application-specific niches, e.g., hardware constraints. By comparing these optimizers to their multi-objective counterparts, we demonstrate that quality diversity NAS in general outperforms multi-objective NAS with respect to quality of solutions and efficiency. We further show how applications and future NAS research can thrive on QDO.}
}

@InProceedings{zhang22,
  title      = {A Tree-Structured Multi-Task Model Recommender},
  author    = {Zhang, Lijun and Liu, Xiao and Guan, Hui},
  pages      = {10/1--12},
  openreview = {SrrgrWs0-g5},
  software   = {https://github.com/zhanglijun95/TreeMTL},
  abstract   = {Tree-structured multi-task architectures have been employed to jointly tackle multiple vision tasks in the context of multi-task learning (MTL).  The major challenge is to determine where to branch out for each task given a backbone model to optimize for both task accuracy and computation efficiency. To address the challenge, this paper proposes a recommender that, given a set of tasks and a convolutional neural network-based backbone model, automatically suggests tree-structured multi-task architectures that could achieve a high task performance while meeting a user-specified computation budget without performing model training. Extensive evaluations on popular MTL benchmarks show that the recommended architectures could achieve competitive task accuracy and computation efficiency compared with state-of-the-art MTL methods. Our tree-structured multi-task model recommender is open-sourced and available at \url{https://github.com/zhanglijun95/TreeMTL}.}
}

@InProceedings{bahrami22,
  title      = {BERT-Sort: A Zero-shot MLM Semantic Encoder on Ordinal Features for AutoML},
  author    = {Bahrami, Mehdi and Chen, Wei-Peng and Liu, Lei and Prasad, Mukul},
  pages      = {11/1--26},
  openreview = {rE2g8Qf6req},
  software   = {https://github.com/marscod/BERT-Sort},
  abstract   = {Data pre-processing is one of the key steps in creating machine learning pipelines for tabular data. One of the common data pre-processing operations implemented in AutoML systems is to encode categorical features as numerical features. Typically, this is implemented using a simple alphabetical sort on the categorical values, using functions such as OrdinalEncoder, LabelEncoder in Scikit-Learn and H2O. However, often there exist semantic ordinal relationships among the categorical values, such as: quality level (i.e., ['very good' \ensuremath{>} 'good' \ensuremath{>} 'normal'\ensuremath{>} 'poor']), or month (i.e., ['Jan'\ensuremath{<} 'Feb' \ensuremath{<} 'Mar']). Such semantic relationships are not exploited by previous AutoML approaches. In this paper, we introduce BERT-Sort, a novel approach to semantically encode ordinal categorical values via zero-shot Masked Language Models (MLM) and apply it to AutoML for tabular data. We created a new benchmark of 42 features from 10 public data sets for sorting categorical ordinal values for the first time, where BERT-Sort significantly improves semantic encoding of ordinal values in comparison to the existing approaches with 27\% improvement. We perform a comprehensive evaluation of BERT-Sort on different public MLMs, such as RoBERTa, XLM and DistilBERT. We also compare the performance of raw data sets against encoded data sets through BERT-Sort in different AutoML platforms including AutoGluon, FLAML, H2O, and MLJAR to evaluate the proposed approach in an end-to-end scenario, where BERT-Sort achieved a performance close to a hard encoded feature. The artifacts of BERT-Sort is available at https://github.com/marscod/BERT-Sort.}
}

@InProceedings{ram22,
  title      = {On the Optimality Gap of Warm-Started Hyperparameter Optimization},
  author    = {Ram, Parikshit},
  pages      = {12/1--14},
  openreview = {SUQgACdBfx5},
  abstract   = {We study the general framework of warm-started hyperparameter optimization (HPO) where we have some source datasets (tasks) where we have already performed HPO, and we wish to leverage the results of these HPO to warm-start the HPO on an unseen target dataset (and perform few-shot HPO). Various meta-learning schemes have been proposed over the last decade (and more) for this problem. In this paper, we theoretically analyse the optimality gap of the hyperparameter obtained via such warm-started few-shot HPO, and provide novel results for multiple existing meta-learning schemes. We show how these results allow us identify situations where certain schemes have advantage over others.}
}

@InProceedings{laube22,
  title      = {What to expect of hardware metric predictors in NAS},
  author    = {Laube, Kevin Alexander and Mutschler, Maximus and Zell, Andreas},
  pages      = {13/1--15},
  openreview = {B__lCvzZglc},
  abstract   = {Modern Neural Architecture Search (NAS) focuses on finding the best performing architectures in hardware-aware settings; e.g., those with an optimal tradeoff of accuracy and latency. Due to many advantages of prediction models over live measurements, the search process is often guided by estimates of how well each considered network architecture performs on the desired metrics. Typical prediction models range from operation-wise lookup tables over gradient-boosted trees and neural networks, with little known information on how they compare. We evaluate 18 different performance predictors on ten combinations of metrics, devices, network types, and training tasks, and find that MLP models are the most promising. We then simulate and evaluate how the guidance of such prediction models affects the subsequent architecture selection. Due to inaccurate predictions, the selected architectures are generally suboptimal, which we quantify as an expected reduction in accuracy and hypervolume. We show that simply verifying the predictions of just the selected architectures can lead to substantially improved results. Under a time budget, we find it preferable to use a fast and inaccurate prediction model over accurate but slow live measurements.}
}

@InProceedings{wan22,
  title      = {Bayesian Generational Population-Based Training},
  author    = {Wan, Xingchen and Lu, Cong and Parker-Holder, Jack and Ball, Philip J. and Nguyen, Vu and Ru, Binxin and Osborne, Michael},
  pages      = {14/1--27},
  openreview = {rx-efyxBSg9},
  abstract   = {Reinforcement learning (RL) offers the potential for training generally capable agents that can interact autonomously in the real world. However, one key limitation is the brittleness of RL algorithms to core hyperparameters and network architecture choice. Furthermore, non-stationarities such as evolving training data and increased agent complexity mean that different hyperparameters and architectures may be optimal at different points of training. This motivates AutoRL, a class of methods seeking to automate these design choices. One prominent class of AutoRL methods is Population-Based Training (PBT), which have led to impressive performance in several large scale settings. In this paper, we introduce two new innovations in PBT-style methods. First, we employ trust-region based Bayesian Optimization, enabling full coverage of the high-dimensional mixed hyperparameter search space. Second, we show that using a generational approach, we can also learn both architectures and hyperparameters jointly on-the-fly in a single training run. Leveraging the new highly parallelizable Brax physics engine, we show that these innovations lead to dramatic performance gains, significantly outperforming the tuned baseline while learning entire configurations on the fly.}
}

@InProceedings{cheng22,
  title      = {ScaleNAS: Multi-Path One-Shot NAS for Scale-Aware High-Resolution Representation},
  author    = {Cheng, Hsin-Pai and Liang, Feng and Li, Meng and Cheng, Bowen and Yan, Feng and Li, Hai and Chandra, Vikas and Chen, Yiran},
  pages      = {15/1--18},
  openreview = {SBxeLI4nEe9},
  abstract   = {Scale variance among different sizes of body parts and objects is a challenging problem for visual recognition tasks. Existing works usually design dedicated backbone or apply Neural architecture Search (NAS) for each task to tackle this challenge. However, existing works impose significant limitations on the design or search space. To solve these problems, we present ScaleNAS, a one-shot learning method for exploring scale-aware representations. ScaleNAS solves multiple tasks at a time by searching multi-scale feature aggregation. ScaleNAS adopts a flexible search space that allows an arbitrary number of blocks and cross-scale feature fusions. To cope with the high search cost incurred by the flexible space, ScaleNAS employs one-shot learning for multi-scale supernet driven by grouped sampling and evolutionary search. Without further retraining, ScaleNet can be directly deployed for different visual recognition tasks with superior performance. We use ScaleNAS to create high-resolution models for two different tasks, ScaleNet-P for human pose estimation and ScaleNet-S for semantic segmentation. ScaleNet-P and ScaleNet-S outperform existing manually crafted and NAS-based methods in both tasks. When applying ScaleNet-P to bottom-up human pose estimation, it surpasses the state-of-the-art HigherHRNet. In particular, ScaleNet-P4 achieves 71.6\% AP on COCO test-dev, achieving new state-of-the-art result.}
}

@InProceedings{salinas22,
  title      = {Syne Tune: A Library for Large Scale Hyperparameter Tuning and Reproducible Research},
  author    = {Salinas, David and Seeger, Matthias and Klein, Aaron and Perrone, Valerio and Wistuba, Martin and Archambeau, Cedric},
  pages      = {16/1--23},
  openreview = {rg-lnb6CQxq},
  abstract   = {We present Syne Tune, a library for large-scale distributed hyperparameter optimization (HPO). Syne Tune's modular  architecture allows users to easily switch between different execution backends to facilitate experimentation and makes it easy to contribute new optimization algorithms. To foster reproducible benchmarking, Syne Tune provides an efficient simulator backend and a benchmarking suite, which are essential for large-scale evaluations of distributed asynchronous HPO algorithms on tabulated and surrogate benchmarks. We showcase these functionalities with a range of state-of-the-art gradient-free optimizers, including multi-fidelity and transfer learning approaches on popular benchmarks from the literature. Additionally, we demonstrate the benefits of Syne Tunefor constrained and multi-objective HPO applications through two use cases: the former considers hyperparameters that induce fair solutions and the latter automatically selects machine types along with the conventional hyperparameters.}
}

@InProceedings{zhu22,
  title      = {DIFER: Differentiable Automated Feature Engineering},
  author    = {Zhu, Guanghui and Xu, Zhuoer and Yuan, Chunfeng and Huang, Yihua},
  pages      = {17/1--17},
  openreview = {BzgyEgPQl9},
  software   = {https://anonymous.4open.science/r/DIFER-3FBC/},
  abstract   = {Feature engineering, a crucial step of machine learning, aims to extract useful features from raw data to improve model performance. In recent years, great efforts have been devoted to Automated Feature Engineering (AutoFE) to replace expensive human labor. However, all existing methods treat AutoFE as an optimization problem over a discrete feature space. Huge search space leads to significant computational overhead. Unlike previous work, we perform AutoFE in a continuous vector space and propose a differentiable method called DIFER in this paper. We first introduce a feature optimizer based on the encoder-predictor-decoder framework, which maps features into the continuous vector space via the encoder, optimizes the embedding along the gradient direction induced by the predictor, and recovers better features from the optimized embedding by the decoder. Based on the feature optimizer, we employ a feature evolution method to search for better features iteratively. Extensive experiments on classification and regression datasets demonstrate that DIFER can significantly outperform the state-of-the-art AutoFE methods in terms of both model performance and computational efficiency. The implementation of DIFER is avaialable on \url{https://anonymous.4open.science/r/DIFER-3FBC/}.}
}

@InProceedings{maile22,
  title      = {When, where, and how to add new neurons to ANNs},
  author    = {Maile, Kaitlin and Rachelson, Emmanuel and Luga, Herv\'e and Wilson, Dennis George},
  pages      = {18/1--12},
  openreview = {BNxx4WUzBl5},
  abstract   = {Neurogenesis in ANNs is an understudied and difficult problem, even compared to other forms of structural learning like pruning. By decomposing it into triggers and initializations, we introduce a framework for studying the various facets of neurogenesis: when, where, and how to add neurons during the learning process. We present the Neural Orthogonality (NORTH*) suite of neurogenesis strategies, combining layer-wise triggers and initializations based on the orthogonality of activations or weights to dynamically grow performant networks that converge to an efficient size. We evaluate our contributions against other recent neurogenesis works across a variety of supervised learning tasks.}
}

@InProceedings{bansal22,
  title      = {Meta-Adapters: Parameter Efficient Few-shot Fine-tuning through Meta-Learning},
  author    = {Bansal, Trapit and Alzubi, Salaheddin and Wang, Tong and Lee, Jay-Yoon and McCallum, Andrew},
  pages      = {19/1--18},
  openreview = {S85lEqIhBec},
  abstract   = {Consistent improvements in the representational capacity of large pre-trained transformers has made it increasingly viable to serve these models as shared priors that can be fine-tuned on a large number of downstream tasks. However, fine-tuning the entire model for every task of interest makes a copy of all the model parameters, rendering such scenarios highly impractical.  Recently introduced Adapter methods propose a promising alternative, one where only a small number of additional parameters are introduced per task specifically for fine-tuning. However, Adapters often require large amounts of task-specific data for good performance and don't work well in data-scarce few-shot scenarios. In this paper, we approach parameter-efficient fine-tuning in few-shot settings from a meta-learning perspective. We introduce Meta-Adapters, which are small blocks of meta-learned adapter layers inserted in a pre-trained model that re-purpose a frozen pre-trained model into a parameter-efficient few-shot learner. Meta-Adapters perform competitively with state-of-the-art few-shot learning methods that require full fine-tuning, while only fine-tuning 0.6\% of the parameters. We evaluate Meta-Adapters along with multiple transfer learning baselines on an evaluation suite of 17 classification tasks and find that they improve few-shot accuracy by a large margin over competitive parameter-efficient methods, while requiring significantly lesser parameters for fine-tuning. Moreover, when comparing few-shot prompting of GPT-3 against few-shot fine-tuning with Meta-Adapters, we find that Meta-Adapters perform  competitively while working with pre-trained transformers that are many orders of magnitude (1590{\texttimes}) smaller in size than GPT-3.}
}

@InProceedings{miao22,
  title      = {Differentiable Architecture Search for Reinforcement Learning},
  author    = {Miao, Yingjie and Song, Xingyou and Co-Reyes, John D and Peng, Daiyi and Yue, Summer and Brevdo, Eugene and Faust, Aleksandra},
  pages      = {20/1--17},
  openreview = {HOZx3iEht15},
  abstract   = {In this paper, we investigate the fundamental question: To what extent are gradient-based neural architecture search (NAS) techniques applicable to RL? Using the original DARTS as a convenient baseline, we discover that the discrete architectures found can achieve up to 250\% performance compared to manual architecture designs on both discrete and continuous action space environments across off-policy and on-policy RL algorithms, at only 3{\texttimes} more computation time. Furthermore, through numerous ablation studies, we systematically verify that not only does DARTS correctly upweight operations during its supernet phrase, but also gradually improves resulting discrete cells up to 30{\texttimes} more efficiently than random search, suggesting DARTS is surprisingly an effective tool for improving architectures in RL.}
}
